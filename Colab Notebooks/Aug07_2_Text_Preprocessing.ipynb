{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrURZpTtZrsLgH/yRO5i6Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 패딩 (Padding)\n","\n","자연어 처리 진행 중 각각의 문장마다 길이가 서로 다름\n","\n","컴퓨터 입장 : 길이가 동일한 문장의 경우에는 하나의 행렬로 묶어서 보게 되고, 그걸로 한번에 처리가 가능\n","\n","다양한 문장 길이를 임의적으로 동일하게 맞춰주는 작업을 패딩 (Padding)"],"metadata":{"id":"DMFH1CFoksxQ"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"wN6mrnWIlLmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_sentence = [['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['put', 'right', 'hand', 'put', 'right', 'hand'], ['give', 'right', 'hand', 'shake', 'shake', 'shake', 'turn'], [], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['put', 'right', 'hand', 'put', 'right', 'hand'], ['give', 'right', 'hand', 'shake', 'shake', 'shake', 'turn'], [], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night']]\n","print(final_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qo3UuTqUlRh7","executionInfo":{"status":"ok","timestamp":1722996350363,"user_tz":-540,"elapsed":318,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"d9c55d02-0c45-4345-85c7-635a69fae8c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['put', 'right', 'hand', 'put', 'right', 'hand'], ['give', 'right', 'hand', 'shake', 'shake', 'shake', 'turn'], [], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night'], ['put', 'right', 'hand', 'put', 'right', 'hand'], ['give', 'right', 'hand', 'shake', 'shake', 'shake', 'turn'], [], ['looby', 'loo', 'looby', 'light'], ['looby', 'loo', 'saturday', 'night']]\n"]}]},{"cell_type":"code","source":["# 정수 인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(final_sentence)\n","aa = tokenizer.texts_to_sequences(final_sentence)\n","print(aa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwC69iOLldp4","executionInfo":{"status":"ok","timestamp":1722996425954,"user_tz":-540,"elapsed":359,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"7ab30e7b-4ca8-4cd4-d535-9614eee3bd67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 1, 6], [1, 2, 7, 8], [9, 3, 4, 9, 3, 4], [10, 3, 4, 5, 5, 5, 11], [], [1, 2, 1, 6], [1, 2, 7, 8], [1, 2, 1, 6], [1, 2, 7, 8], [9, 3, 4, 9, 3, 4], [10, 3, 4, 5, 5, 5, 11], [], [1, 2, 1, 6], [1, 2, 7, 8]]\n"]}]},{"cell_type":"code","source":["# 가장 길이가 긴 문장이 길이가 얼마나 되는지\n","\n","long_sentence = max(len(item) for item in aa)\n","\n","print(long_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5gdf_s5ltdC","executionInfo":{"status":"ok","timestamp":1722996529678,"user_tz":-540,"elapsed":312,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"ac7f8a6c-45f3-41b9-91c7-3e43c0aee4fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7\n"]}]},{"cell_type":"code","source":["# 모든 문장의 길이를 가장 길이가 긴 문장의 크기만큼 맞춰주기\n","# 임의의(가상의) 단어가 있다고 가정한 후 => 이 단어의 인덱스는 0\n","# ex) 최대 길이가 4일때, 문장의 길이가 4보다 짧으면 4가 될때까지 0으로 채우는...\n","\n","for item in aa:\n","  while len(item) < long_sentence:\n","    item.append(0)\n","\n","n = np.array(aa)\n","n\n","\n","# 패딩 : Data에 특정한 값을 넣어서 data의 shape(크기)를 조정하는 것!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kXD6TImmJcC","executionInfo":{"status":"ok","timestamp":1722996708756,"user_tz":-540,"elapsed":379,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"25b30cb8-c408-4e5f-e22f-34ff6ddc4123"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 9,  3,  4,  9,  3,  4,  0],\n","       [10,  3,  4,  5,  5,  5, 11],\n","       [ 0,  0,  0,  0,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 9,  3,  4,  9,  3,  4,  0],\n","       [10,  3,  4,  5,  5,  5, 11],\n","       [ 0,  0,  0,  0,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Keras의 pad_sequences"],"metadata":{"id":"xN0bveVHnL8m"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"8jZmPPeWnVgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위에 있는 aa를 원상태로 복구\n","aa = tokenizer.texts_to_sequences(final_sentence)\n","print(aa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P89latYWnZSC","executionInfo":{"status":"ok","timestamp":1722996958770,"user_tz":-540,"elapsed":333,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"5be05508-8507-4977-f152-c40d4b07804c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 1, 6], [1, 2, 7, 8], [9, 3, 4, 9, 3, 4], [10, 3, 4, 5, 5, 5, 11], [], [1, 2, 1, 6], [1, 2, 7, 8], [1, 2, 1, 6], [1, 2, 7, 8], [9, 3, 4, 9, 3, 4], [10, 3, 4, 5, 5, 5, 11], [], [1, 2, 1, 6], [1, 2, 7, 8]]\n"]}]},{"cell_type":"code","source":["# kerasPadding = pad_sequences(aa) # 데이터의 앞에서부터 0을 채움\n","kerasPadding = pad_sequences(aa, padding='post') # 데이터의 뒤에 0을 채움\n","kerasPadding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsGVwB5SnyMc","executionInfo":{"status":"ok","timestamp":1722997067163,"user_tz":-540,"elapsed":355,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"59b775ac-d2ec-47e6-f378-61a552c9cbfd"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 9,  3,  4,  9,  3,  4,  0],\n","       [10,  3,  4,  5,  5,  5, 11],\n","       [ 0,  0,  0,  0,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0],\n","       [ 9,  3,  4,  9,  3,  4,  0],\n","       [10,  3,  4,  5,  5,  5, 11],\n","       [ 0,  0,  0,  0,  0,  0,  0],\n","       [ 1,  2,  1,  6,  0,  0,  0],\n","       [ 1,  2,  7,  8,  0,  0,  0]], dtype=int32)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 기준을 최대 길이로 할 필요가 없는 경우에 'maxlen'을 이용해서 원하는 길이 조절 가능\n","# 원하는 길이보다 긴 문장이 있는 경우, 데이터 해당하는 갯수만큼 손실\n","# 기본적으로 문장의 맨 앞에서부터 차례대로 데이터의 손실이 발생\n","# 데이터 손실을 맨 뒤에서부터 받고 싶다면, truncating='post' 옵션을 추가해서...!\n","kerasPadding2 = pad_sequences(aa, padding='post', truncating='post', maxlen=3)\n","kerasPadding2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rw0H8WPTn1Fa","executionInfo":{"status":"ok","timestamp":1722997673976,"user_tz":-540,"elapsed":358,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"43c2f33c-0a86-4c08-b3b4-d41f7f5288ac"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"markdown","source":["# One-Hot Encoding (원-핫 인코딩)\n","\n","컴퓨터 : 문자보다 숫자를 더 잘 처리함\n","\n","문자를 숫자로 바꾸는 방법 중 하나\n","\n","데이터를 수많은 0과 한개의 1값으로 데이터를 구별하는 인코딩\n","\n","단어 집합의 크기를 vector의 차원(1차원)으로 지정하고, 표현하고자 하는 단어의 인덱스에 1로 부여 / 다른 단어에는 0을 부여하는 단어의 벡터\n","\n","- 벡터(Vector) : 공간에서 크기와 방향을 가진 것\n","  \n","  순서가 존재하는 리스트 !\n","\n","ex) [190.3, 81.4] != [81.4, 190.3]"],"metadata":{"id":"A6B4wZcxppzw"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7qXcdnDqlqx","executionInfo":{"status":"ok","timestamp":1722997705920,"user_tz":-540,"elapsed":6931,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"7302e803-82e3-4b2f-8a13-242ba430b851"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt"],"metadata":{"id":"_Df7wktiqm-_","executionInfo":{"status":"ok","timestamp":1722997737615,"user_tz":-540,"elapsed":338,"user":{"displayName":"이영훈","userId":"04987503508550913296"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["okt = Okt()\n","word_token = okt.morphs('오늘은 어째서 금요일이 아니죠?')\n","print(word_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNUPGpDeqwNs","executionInfo":{"status":"ok","timestamp":1722997766373,"user_tz":-540,"elapsed":12839,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"d0bb6818-508d-40b1-dfd4-21d66b55c918"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['오늘', '은', '어째서', '금요일', '이', '아니죠', '?']\n"]}]},{"cell_type":"code","source":["# 나눠진 단어에 각각 index 부여하기 (0부터 시작)\n","# enumerate를 통해서 자동으로 0부터 인덱스가 부여되도록\n","word_index = {w : i for i, w in enumerate(word_token)}\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dxv2OGglq0OG","executionInfo":{"status":"ok","timestamp":1722997889911,"user_tz":-540,"elapsed":349,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"9a597771-ee8e-46ce-cbc4-5771a06e0631"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오늘': 0, '은': 1, '어째서': 2, '금요일': 3, '이': 4, '아니죠': 5, '?': 6}\n"]}]},{"cell_type":"code","source":["# 토큰을 입력하면, 토큰에 대한 원-핫 벡터를 뽑아내는 함수\n","def ohe(w, word_index):\n","  ohVector = [0] * len(word_index) # 위에서 만든 index의 길이만큼 0을 채운 list를 생성\n","  index = word_index[w]   # 파라미터로 넣은 단어를 넣은 변수\n","  ohVector[index] = 1     # 해당변수가 있는 위치를 1값으로 변경\n","  return ohVector\n"],"metadata":{"id":"czn53N7mrVhi","executionInfo":{"status":"ok","timestamp":1722998062153,"user_tz":-540,"elapsed":361,"user":{"displayName":"이영훈","userId":"04987503508550913296"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# '금요일' 이라는 단어의 원-핫 벡터\n","ohe('금요일', word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4N_bfkyjr-Ao","executionInfo":{"status":"ok","timestamp":1722998106137,"user_tz":-540,"elapsed":332,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"6aef6435-b3aa-46b5-8881-53026643a431"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 1, 0, 0, 0]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# Keras를 이용한 원-핫 인코딩\n"],"metadata":{"id":"vhIcjtYjyDQ0"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"sf3bJnJayIY_","executionInfo":{"status":"ok","timestamp":1722999707435,"user_tz":-540,"elapsed":341,"user":{"displayName":"이영훈","userId":"04987503508550913296"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["text = \"곰 세 마리가 한 집에 있어 아빠 곰 엄마 곰 애기 곰\"\n","\n","# 빈도수 기준으로 단어 집합 만들기\n","tokenizer = Tokenizer()\n","# fit_on_texts 함수가 리스트 형태의 입력을 요구함\n","tokenizer.fit_on_texts([text])\n","print(tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MM3spzFnyPnt","executionInfo":{"status":"ok","timestamp":1722999886957,"user_tz":-540,"elapsed":350,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"3ff63295-314a-472e-ca81-0e5af2f92c5c"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["{'곰': 1, '세': 2, '마리가': 3, '한': 4, '집에': 5, '있어': 6, '아빠': 7, '엄마': 8, '애기': 9}\n"]}]},{"cell_type":"code","source":["# 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트를 하나 생성\n","text2 = \"아빠 곰 세 마리가\"\n","\n","# texts_to_sequences는 리스트 [\"아빠 곰 세마리가\"]를 입력 (리스트 형태의 입력을 요구함)\n","# 이 함수는 입력 리스트의 각 요소(텍스트)를 숫자로 변환해서 리스트 형태로 반환\n","# 이 함수의 반환 값이 [[7, 1, 2, 3]]이므로, 이중 리스트 형태\n","# 즉, 리스트 안의 텍스트를 변환하여 리스트로 return하기 때문에,\n","#   단일 텍스트 변환 결과를 얻기 위해서 0번째 요소를 가져왔음\n","\n","bear = tokenizer.texts_to_sequences([text2])[0]\n","print(bear)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6AxkMzHy885","executionInfo":{"status":"ok","timestamp":1723000170943,"user_tz":-540,"elapsed":336,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"3407c3ff-9cfe-43b1-8e9d-20ebf455f3a9"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[7, 1, 2, 3]\n"]}]},{"cell_type":"code","source":["# to_categorical() : keras에서 원-핫 인코딩하는 함수\n","cate = to_categorical(bear)\n","print(cate)\n","\n","# [7, 1, 2, 3] 을 표현한 결과\n","# 1. 단어 집합의 인덱스 시작 숫자는 1\n","# 2. 컴퓨터는 0부터 시작\n","# 3. 각 list의 0번째 자리는 임의적으로 0으로 채워두고\n","# 4. 각각의 인덱스에 해당하는 자리의 값이 1로 바뀌도록!!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbT-xS-X0Jgh","executionInfo":{"status":"ok","timestamp":1723000217782,"user_tz":-540,"elapsed":328,"user":{"displayName":"이영훈","userId":"04987503508550913296"}},"outputId":"5cf2f4bd-3173-4520-de2b-89100d915337"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["# 장점 :\n","\n","  문자데이터 경우에는 컴퓨터가 처리하기 쉽게 숫자로 변환\n","\n","# 단점 :\n","\n","  1. 단어의 갯수가 늘어날수록, 벡터를 저장하기 위한 공간이 늘어남 (= 벡터의 차원이 늘어남)\n","  2. 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점\n","  \n","  (추천 시스템의 경우)\n","\n","  특정 검색어에 대해서 유사 단어에 대한 결과도 함께 보여줄 수 있어야 하는데, 단어간의 유사성을 계산할 수 없다면, 연관 검색어를 보여줄 수 없을 것!\n","\n","  이러한 단점을 해결하기 위해서 단어의 잠재 의미를 반영해서 다차원 공간에 벡터화 하는 기법을 사용할건데 => 대표적으로 예측 기반으로 벡터화하는 Word2Vec(머신러닝 모델)이 있음"],"metadata":{"id":"RpFJ7Id60ydz"}},{"cell_type":"code","source":[],"metadata":{"id":"DpZIg0M_0T8f"},"execution_count":null,"outputs":[]}]}